{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import keras\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "\n",
    "image_width = 480\n",
    "image_height = 480\n",
    "image_depth = 1\n",
    "\n",
    "num_classes = 3\n",
    "categories_name = ['Background', 'Cosmic', 'Beam']\n",
    "categories_value = [0, 1, 2]\n",
    "\n",
    "# Total: 10620\n",
    "n_training = 30\n",
    "training_feature_file = \"input_files/training/feature_w.csv\"\n",
    "training_label_file = \"input_files/training/label_w.csv\"\n",
    "\n",
    "# Total: 1320\n",
    "n_validation = 10\n",
    "validation_feature_file = \"input_files/validation/feature_w.csv\"\n",
    "validation_label_file = \"input_files/validation/label_w.csv\"\n",
    "\n",
    "# Total: 1330\n",
    "n_testing = 10\n",
    "testing_feature_file = \"input_files/testing/feature_w.csv\"\n",
    "testing_label_file = \"input_files/testing/label_w.csv\"\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(feature_file, label_file):\n",
    "    with open(feature_file, \"r\") as csv1, open(label_file, \"r\") as csv2:\n",
    "        reader1 = csv.reader(csv1)\n",
    "        reader2 = csv.reader(csv2)\n",
    "        # Skip the header row\n",
    "        next(reader1)\n",
    "        next(reader2)\n",
    "        for row1, row2 in zip(reader1, reader2):\n",
    "            array_row1 = np.array(row1, dtype=np.float)\n",
    "            array_row2 = np.array(row2, dtype=np.int)\n",
    "            yield array_row1, array_row2\n",
    "\n",
    "def preprocess_x(x):\n",
    "    \"\"\"\n",
    "    Feature is the adc values; scale it such that each value is between 0 and 1\n",
    "    \"\"\"\n",
    "    x_max = np.max(x)\n",
    "    x = x/x_max\n",
    "    return x.reshape(1, image_width, image_height, image_depth)\n",
    "\n",
    "def preprocess_y(y):\n",
    "    return np_utils.to_categorical(y, num_classes=num_classes).reshape(1, image_width, image_height, num_classes)\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Although sequence are a safer way to do multiprocessing, \n",
    "    use_multiprocessing=True in fit_generator is currently not supported here.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_file, label_file, max_index=1, batch_size=1):\n",
    "        self.feature_file = feature_file\n",
    "        self.label_file = label_file\n",
    "        self.max_index = max_index\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The number of batches in a epoch.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(self.max_index / float(self.batch_size)))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one batch of data at 'index', which is the position of the batch in the Sequence.\n",
    "        \"\"\"\n",
    "        print(\"Current index: {}\\n\".format(index))\n",
    "        index = index * self.batch_size\n",
    "        print(\"Real index: {}\\n\".format(index))\n",
    "        \n",
    "        if self.batch_size >= self.max_index:\n",
    "            self.rows = np.arange(0, self.max_index)\n",
    "        if index + self.batch_size >= self.max_index:\n",
    "            self.rows = np.arange(0, self.max_index - index)\n",
    "        else:\n",
    "            self.rows = np.arange(0, self.batch_size)\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(self.rows)\n",
    "                \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Update after each epoch.\n",
    "        \"\"\"\n",
    "        self.rows = np.arange(0, min(self.batch_size, self.max_index))\n",
    "        \n",
    "        self.reader1 = csv.reader(open(self.feature_file, \"r\"))\n",
    "        self.reader2 = csv.reader(open(self.label_file, \"r\"))\n",
    "\n",
    "        # Skip the header row and count coln\n",
    "        self.n_col = len(next(self.reader1))\n",
    "        next(self.reader2)   \n",
    "\n",
    "    def __data_generation(self, rows):\n",
    "        \"\"\"Generates data containing batch_size samples\"\"\"\n",
    "        samples = np.zeros((len(rows), image_width, image_height, image_depth))\n",
    "        targets = np.zeros((len(rows), image_width, image_height, num_classes))\n",
    "        for j, _ in enumerate(rows):\n",
    "            for row1, row2 in zip(self.reader1, self.reader2):\n",
    "                array_row1 = np.array(row1, dtype=np.float)\n",
    "                samples[j,:,:,:] = preprocess_x(array_row1)\n",
    "                next(self.reader1)\n",
    "\n",
    "                array_row2 = np.array(row2, dtype=np.int)\n",
    "                targets[j,:,:,:] = preprocess_y(array_row2)\n",
    "                next(self.reader2)\n",
    "                    \n",
    "        return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_class_weights(y):\n",
    "    \"\"\"\n",
    "    Returns the weights for each class based on the frequencies of the samples\n",
    "    For 3 classes with classA:10%, classB:50% and classC:40%, the weights will be:\n",
    "    {0:5, 1:1, 2:1.25}\n",
    "    This means that if you miss-classify classA the loss will be 5x more than miss-classifying classB and so on...\n",
    "    \"\"\"\n",
    "    counter = Counter(y)\n",
    "\n",
    "    majority = max(counter.values())\n",
    "    return {cls: float(majority) / count for cls, count in counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "iter_data = get_data(training_feature_file, training_label_file)\n",
    "weights = [[],[],[]]\n",
    "for X, y in tqdm(iter_data):\n",
    "    class_weights = get_class_weights(y)\n",
    "    for index, weight in class_weights.items():\n",
    "        weights[index].append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weights, plot_name):\n",
    "    ranges = [(0,2), (0,50), (0, 3500)]\n",
    "    fig, axes = plt.subplots(1, len(categories_value), figsize=(18,5), facecolor='w')\n",
    "    for index, value in enumerate(categories_value):\n",
    "        ax = axes[index]\n",
    "        ax.hist(weights[index], 100, range=ranges[index], color='green', alpha=0.75)\n",
    "        ax.set_title(categories_name[index], fontsize=20, fontname='Georgia',fontweight='bold')\n",
    "        ax.set_xlabel(\"Weight\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "        ax.set_ylabel(\"Count\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "        \n",
    "        _, max_ = ax.get_ylim()\n",
    "        median = np.median(weights[index])\n",
    "        ax.axvline(median, color='k', linestyle='dashed', linewidth=2)\n",
    "        ax.text(median + median/10, max_ - max_/10, 'Median: {:.2f}'.format(median), \n",
    "                fontsize=12, fontweight=1000, color='k')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(plot_name, bbox_inches='tight')\n",
    "    \n",
    "plot_weights(weights, \"plots/weights.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = [1.0, 25.0, 415.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(feature_image):\n",
    "    fig, ax0 = plt.subplots(1, 1, figsize=(10,5), facecolor='w')\n",
    "    c0 = ax0.imshow(feature_image, cmap='ocean_r',interpolation='none', origin='lower',\n",
    "                    vmin=0.0, vmax=1.0)\n",
    "    fig.colorbar(c0, ax=ax0)\n",
    "    ax0.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_ylabel(\"TDC\", fontsize=1, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_title('Feature', fontsize=20,fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_label(label_image):\n",
    "    fig, ax0 = plt.subplots(1, 1, figsize=(10,5), facecolor='w')\n",
    "    c0 = ax0.imshow(feature_image, cmap='gist_heat_r',interpolation='none', origin='lower',\n",
    "                    vmin=0.0, vmax=1.0)\n",
    "    fig.colorbar(c0, ax=ax0)\n",
    "    ax0.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_ylabel(\"TDC\", fontsize=1, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_title('Label', fontsize=20,fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen = DataGenerator(training_feature_file, training_label_file)\n",
    "X, y = my_gen.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "feature_image = X[0].reshape(image_width, image_height)\n",
    "print(feature_image.shape)\n",
    "plot_feature(feature_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "image_label = np.argmax(y[0], axis=2)\n",
    "print(image_label.shape)\n",
    "plot_label(image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_label(feature_image, label_image, plot_name):\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(20,7), facecolor='w')\n",
    "    c0 = ax0.imshow(feature_image, cmap='winter_r',interpolation='none', origin='lower', \n",
    "                    norm=LogNorm(vmin=1.0, vmax=abs(feature_image).max()))\n",
    "    fig.colorbar(c0, ax=ax0)\n",
    "    ax0.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_ylabel(\"TDC\", fontsize=1, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_title('Feature', fontsize=20,fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    c1= ax1.imshow(label_image,cmap='gist_heat_r',interpolation='none', origin='lower', \n",
    "                   vmin=abs(label_image).min(), vmax=abs(label_image).max())\n",
    "    fig.colorbar(c1, ax=ax1)\n",
    "    ax1.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax1.set_ylabel(\"TDC\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax1.set_title('Label', fontsize=20, fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "def plot_categories(feature_image, label_image, plot_name):\n",
    "    fig, axes = plt.subplots(1, len(categories_value), figsize=(18,12), facecolor='w')\n",
    "    for index, value in enumerate(categories_value):\n",
    "        ax = axes[index]\n",
    "        mask = (label_image == value)\n",
    "        ax.imshow(feature_image*mask, cmap='winter_r',interpolation='none', origin='lower', \n",
    "                  norm=LogNorm(vmin=1.0, vmax=abs(feature_image).max()))\n",
    "        ax.set_title(categories_name[index], fontsize=20, fontname='Georgia',fontweight='bold')\n",
    "    plt.show()\n",
    "    fig.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "iter_data = get_data(training_feature_file, training_label_file)\n",
    "next(iter_data)\n",
    "next(iter_data)\n",
    "X, y = next(iter_data)\n",
    "\n",
    "feature_image = X.reshape(image_width, image_height)\n",
    "label_image = y.reshape(image_width, image_height)\n",
    "plot_feature_label(feature_image, label_image, 'plots/feature_label.pdf')\n",
    "plot_categories(feature_image, label_image, 'plots/bkg_cosmic_beam.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.activations import softmax\n",
    "\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    # First layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    # Second layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def get_unet(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n",
    "    # Vontracting path\n",
    "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = Dropout(dropout*0.5)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    # Expansive path\n",
    "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax') (c9)        \n",
    "    \n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "def weighted_loss(num_classes, coefficients):\n",
    "    \"\"\"\n",
    "    Gets weighted categorical cross entropy.\n",
    "    Use this loss function with median frequency coefficients weights for class balance.\n",
    "    \"\"\"\n",
    "    \n",
    "    coefficients = tf.constant(coefficients)\n",
    "    num_classes = tf.constant(num_classes)\n",
    "\n",
    "    def loss(labels, logits):\n",
    "        with tf.name_scope('loss_1'):\n",
    "            logits = tf.reshape(logits, (-1, num_classes))\n",
    "            epsilon = tf.constant(value=1e-10)\n",
    "\n",
    "            logits = logits + epsilon\n",
    "            # consturct one-hot label array\n",
    "            labels = tf.to_float(tf.reshape(labels, (-1, num_classes)))\n",
    "            softmax = tf.nn.softmax(logits)\n",
    "            \n",
    "            cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax + epsilon), \n",
    "                                                       coefficients), reduction_indices=[1])\n",
    "            cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\n",
    "            tf.add_to_collection('losses', cross_entropy_mean)\n",
    "            loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "            #loss = cross_entropy_mean\n",
    "        return loss\n",
    "    return loss\n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate per-step mean Intersection-Over-Union (mIOU).\n",
    "    Computes the IOU for each semantic class and then computes the average over classes.\n",
    "    \"\"\"\n",
    "    score, up_opt = tf.metrics.mean_iou(y_true, y_pred, num_classes)\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    with tf.control_dependencies([up_opt]):\n",
    "        score = tf.identity(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input((image_height, image_width, image_depth))\n",
    "model = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss=weighted_loss(num_classes, coefficients),\n",
    "              metrics = [mean_iou])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "# Stop training when a monitored quantity has stopped improving after 20 epochs\n",
    "early_stop = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.3, patience=3, cooldown=3, verbose=1)\n",
    "\n",
    "# Save the best model after every epoch\n",
    "check_point = ModelCheckpoint(filepath='saved_models/model_and_weights.hdf5', \n",
    "                              save_best_only=True, verbose=1)\n",
    "\n",
    "train_gen = DataGenerator(feature_file=training_feature_file, \n",
    "                           label_file=training_label_file,  \n",
    "                           max_index=n_training, \n",
    "                           batch_size=batch_size)\n",
    "\n",
    "val_gen = DataGenerator(feature_file=validation_feature_file, \n",
    "                         label_file=validation_label_file, \n",
    "                         max_index=n_validation, \n",
    "                         batch_size=batch_size)\n",
    "    \n",
    "test_gen = DataGenerator(feature_file=testing_feature_file, \n",
    "                          label_file=testing_label_file, \n",
    "                          max_index=n_testing, \n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit_generator(train_gen, \n",
    "                              steps_per_epoch = n_training//batch_size,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps= n_validation//batch_size,\n",
    "                              verbose=1,\n",
    "                              callbacks=[check_point, early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for IoU\n",
    "plt.plot(history.history['mean_iou'])\n",
    "plt.plot(history.history['val_mean_iou'])\n",
    "plt.title('Mean IoU')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.savefig('plots/model_accuracy.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Weighted loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.savefig('plots/model_loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the best classification accuracy on the validation set\n",
    "model.load_weights('saved_models/model_and_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_gen_pred = model.predict_generator(test_gen)\n",
    "#print(test_gen_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data = iter(get_data(testing_feature_file, testing_label_file))\n",
    "X, y = next(iter_data)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_procesed = preprocess_x(X)\n",
    "print(X_procesed.shape)\n",
    "predictions = model.predict_on_batch(X_procesed)\n",
    "print(predictions.shape)\n",
    "\n",
    "y_reshaped = preprocess_y(y)\n",
    "print(y_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_max = np.argmax(predictions, axis=3)\n",
    "print(prediction_max.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(y_true, y_pred, epsilon=1e-6):\n",
    "    \n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true + y_pred)\n",
    "\n",
    "    return ((2. * intersection + epsilon)/(union + epsilon))\n",
    "\n",
    "def average_intersection_over_union(y_true, y_pred, n_classes=num_classes):\n",
    "    \"\"\"\n",
    "    Average over classes and batch\n",
    "    \"\"\"\n",
    "    n_preds = predictions.shape[0]\n",
    "    print('Number of validation samples IoU evaulated on: {}'.format(n_preds))\n",
    "    \n",
    "    total_iou = 0\n",
    "    for c in range(n_classes):\n",
    "        average_over_batch_iou = intersection_over_union(y_true[:,:,:,c], y_pred[:,:,:,c])/n_preds\n",
    "        print('Average IoU for {} is: {:.3f}'.format(categories_name[c], average_over_batch_iou))\n",
    "        total_iou += average_over_batch_iou\n",
    "        \n",
    "    print('Global average IoU is: {:.3f}'.format(total_iou/n_classes))\n",
    "\n",
    "average_intersection_over_union(y_reshaped, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_label_prediction(feature_image, label_image, prediction_image, plot_name):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(20,5), facecolor='w')\n",
    "    c0 = ax0.imshow(feature_image, cmap='winter_r',interpolation='none', origin='lower', \n",
    "                    norm=LogNorm(vmin=1.0, vmax=abs(feature_image).max()))\n",
    "    fig.colorbar(c0, ax=ax0)\n",
    "    ax0.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_ylabel(\"TDC\", fontsize=1, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_title('Feature', fontsize=20,fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    c1= ax1.imshow(label_image,cmap='gist_heat_r',interpolation='none', origin='lower', \n",
    "                   vmin=abs(label_image).min(), vmax=abs(label_image).max())\n",
    "    fig.colorbar(c1, ax=ax1)\n",
    "    ax1.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax1.set_ylabel(\"TDC\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax1.set_title('Label', fontsize=20, fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    c2= ax2.imshow(prediction_image, cmap='gist_heat_r',interpolation='none', origin='lower', \n",
    "                   vmin=abs(prediction_image).min(), vmax=abs(prediction_image).max())\n",
    "    fig.colorbar(c2, ax=ax2)\n",
    "    ax2.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax2.set_ylabel(\"TDC\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax2.set_title('Prediction', fontsize=20, fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "def plot_data_prediction(data_image, prediction_image, plot_name):\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(20,7), facecolor='w')\n",
    "    c0 = ax0.imshow(data_image, cmap='winter_r',interpolation='none', origin='lower', \n",
    "                    norm=LogNorm(vmin=1.0, vmax=abs(data_image).max()))\n",
    "    fig.colorbar(c0, ax=ax0)\n",
    "    ax0.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_ylabel(\"TDC\", fontsize=1, fontname='Georgia',fontweight='bold')\n",
    "    ax0.set_title('Data', fontsize=20,fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    c1= ax1.imshow(prediction_image, cmap='gist_heat_r',interpolation='none', origin='lower', \n",
    "                   vmin=abs(prediction_image).min(), vmax=abs(prediction_image).max())\n",
    "    fig.colorbar(c1, ax=ax1)\n",
    "    ax1.set_xlabel(\"Global wire no.\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax1.set_ylabel(\"TDC\", fontsize=15, fontname='Georgia',fontweight='bold')\n",
    "    ax1.set_title('Prediction', fontsize=20, fontname='Georgia',fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "feature_image = X.reshape(image_width, image_height)\n",
    "label_image = y.reshape(image_width, image_height)\n",
    "prediction_image = prediction_max.reshape(image_width, image_height)\n",
    "plot_feature_label_prediction(feature_image, label_image, prediction_image, 'plots/feature_label_prediction.pdf')\n",
    "\n",
    "plot_data_prediction(feature_image, prediction_image, 'plots/data_prediction.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "myML-CPU",
   "language": "python",
   "name": "myml-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
